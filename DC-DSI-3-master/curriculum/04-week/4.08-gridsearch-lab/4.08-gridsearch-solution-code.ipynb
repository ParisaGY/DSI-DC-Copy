{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Lab Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data and conduct an exploratory data analysis.\n",
    "# Resolve any data issues you identify and articulate why you \n",
    "# did what you did.\n",
    "\n",
    "sf_crime = pd.read_csv('datasets/sf_crime_train.csv')\n",
    "sf_crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is a column that is is a Datetime and I want to check and see if it is currently an object\n",
    "sf_crime.Dates.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create column for hour, month, and year from 'Dates' column.\n",
    "- Hint: pd.to_datetime may be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.datetime was not helpful\n",
    "sf_time = pd.DataFrame(sf_crime['Dates'].str.split(' ',1).tolist(),columns = ['date','time'])\n",
    "# sf_time is a dataframe where the Date and time are in separate columns\n",
    "\n",
    "\n",
    "sf_date = pd.DataFrame(sf_time['date'].str.split('/').tolist(),columns = ['month','day','year'])\n",
    "# sf_date is a dataframe where all the month, day and year are all in separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge data frames with individual time values back onto main df\n",
    "sf_crime = sf_crime.merge(sf_date, left_index = True, right_index = True,how = 'outer')\n",
    "sf_crime = sf_crime.merge(sf_time, left_index = True, right_index = True,how = 'outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check out Currnet dictionary if you are interested\n",
    "sf_crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dropping columsn where time is expressed in human speak\n",
    "sf_crime.drop(['Dates','date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18000.000000</td>\n",
       "      <td>18000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-122.423639</td>\n",
       "      <td>37.768466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.026532</td>\n",
       "      <td>0.024391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-122.513642</td>\n",
       "      <td>37.708154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-122.434199</td>\n",
       "      <td>37.753838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-122.416949</td>\n",
       "      <td>37.775608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-122.406539</td>\n",
       "      <td>37.785390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-122.365565</td>\n",
       "      <td>37.819923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  X             Y\n",
       "count  18000.000000  18000.000000\n",
       "mean    -122.423639     37.768466\n",
       "std        0.026532      0.024391\n",
       "min     -122.513642     37.708154\n",
       "25%     -122.434199     37.753838\n",
       "50%     -122.416949     37.775608\n",
       "75%     -122.406539     37.785390\n",
       "max     -122.365565     37.819923"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''I know Matt messed with some value counts so lets see what they are.'''\n",
    "#sf_crime['Dates'].value_counts() '''Dates look good'''\n",
    "\n",
    "#sf_crime['Category'].value_counts()\n",
    "'''1 Trespassing, all others are trespass,  1 Assualt because someone can't spell.'''\n",
    "\n",
    "#sf_crime['Descript'].value_counts()\n",
    "'''data is too diverse, keywords are going to be what matters here'''\n",
    "\n",
    "#sf_crime['DayOfWeek'].value_counts()\n",
    "'''all days off week are there'''\n",
    "\n",
    "#sf_crime['PdDistrict'].value_counts()\n",
    "'''Values look good'''\n",
    "\n",
    "#sf_crime['Resolution'].value_counts()\n",
    "''' 1 non prosecuted.  Seems legit'''\n",
    "\n",
    "sf_crime[['X','Y']].describe()\n",
    "'''all coordinates appear to be relative'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Category, Descript, DayOfWeek, PdDistrict, Resolution, Address, X, Y, month, day, year, time]\n",
       "Index: []"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figuring out where that wrong data exists in the DataFrame\n",
    "sf_crime[sf_crime['Category'] == 'ASSUALT']\n",
    "# rows 2750 and 4330\n",
    "sf_crime[sf_crime['Category'] == 'TRESPASSING']\n",
    "# row 5519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Issues with data are small enough to be manually changes\n",
    "sf_crime.set_value(2750, 'Category', 'ASSAULT')\n",
    "sf_crime.set_value(4330, 'Category', 'ASSAULT')\n",
    "sf_crime.set_value(5519, 'Category', 'TRESPASS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This was probably not the most optimal efficient way to clean this.  If there was an associated dictionary I would have used it to identify and replace any values that were not represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build a logit model predicting violent crime versus non-violent crime versus non-crimes.\n",
    "\n",
    "##### Non-Violent Crimes: \n",
    "bad checks, bribery, drug/narcotic, drunkenness, embezzlement, forgery/counterfeiting, fraud, gambling, liquor, loitering, trespass.\n",
    "\n",
    "#### Non-Crimes: \n",
    "non-criminal, runaway, secondary codes,  suspicious occ, warrants.\n",
    "\n",
    "#### Violent Crimes: \n",
    "everything else.\n",
    "\n",
    "\n",
    "\n",
    "##### Hint: What type of model do you need here? What should your \"baseline\" category be?\n",
    "Multiclass regression.  Our Baseline will probably be Violent as that is the left-over group so to speak.\n",
    "However, it would naturally make sence to set our baseline be the class that has the most observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First i'll need to convert sub categories into overlaying categories.\n",
    "\n",
    "zeros = ['non-criminal', 'runaway', 'secondary codes', 'suspicious occ', 'warrants']\n",
    "ones  = ['bad checks', 'bribery', 'drug/narcotic', 'drunkenness', 'embezzlement', 'forgery/counterfeiting', 'fraud', \n",
    "         'gambling','liquor', 'loitering', 'trespass', 'other offenses']\n",
    "#twos  = all other things  \n",
    "\n",
    "# Empty list to append values into\n",
    "crime_cat = []\n",
    "#iterate through sf_crime Category\n",
    "for crime in sf_crime['Category']:\n",
    "    # convert values to lower\n",
    "    crime = crime.lower()\n",
    "    # checks list of sub categories\n",
    "    if crime in zeros:\n",
    "        # appends the overlaying category\n",
    "        crime_cat.append('non-crime')\n",
    "    elif crime in ones:\n",
    "        crime_cat.append('non-violent')\n",
    "    else:\n",
    "        crime_cat.append('violent')\n",
    "        \n",
    "# take that list and add it to the DF\n",
    "sf_crime['cat_number'] = crime_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# also going to convert DayOfWeek, PdDistrict and Resolution to dummy variables.\n",
    "dummies = pd.get_dummies(sf_crime[['DayOfWeek','PdDistrict','Resolution']], drop_first = True)\n",
    "\n",
    "# Merge the dataframe result back onto the original dataframe\n",
    "sf_crime = sf_crime.merge(dummies, left_index = True, right_index = True,how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a logit model predicting violent crime vs. non-violent crime vs. non-crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_crime.head(0)\n",
    "# Dropping all the categorical values the I don't think will be relevant or have been converted to dummies for X\n",
    "X = sf_crime.drop(['Category','Descript','DayOfWeek','PdDistrict','Resolution','Address','X','Y','cat_number'], axis = 1)\n",
    "y = sf_crime['cat_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit model with five folds and lasso regularization\n",
    "# use Cs=15 to test a grid of 15 distinct parameters\n",
    "# remember: Cs describes the inverse of regularization strength\n",
    "logreg_cv = LogisticRegressionCV(solver='liblinear', Cs =[1,5,10], cv =5, penalty='l1' ) # update inputs here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a reference table for using Regularization.\n",
    "- Regularization - Strength of our regularization\n",
    "- C  -  Larger values Specify stronger regularization\n",
    "- Cs -  Smaller values specify stronger regularization. (Inverse of C : 1 over c)\n",
    "- Area - Refering do those Diamonds and Circles when visualizing Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Regularization| Decrease | Increase |\n",
    "|--------------|----------|----------|\n",
    "|Penalty       | Decrease | Increase |\n",
    "|C             | Increase | Decrease |\n",
    "|Cs            | Decrease | Increase |\n",
    "|Area          | Increase | Decrease |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solver = Algorithm used for Optimization.  \n",
    "    - Newton-cg - Handles Multinomial Loss, L2 only\n",
    "    - Sag - Handles Multinomial Loss, Large Datasets, L2 Only, Works best on sclaed data\n",
    "    - lbfgs - Handles Multinomial Loss, L2, Only\n",
    "    - Liblinear - Small Datasets, no Warm Starts\n",
    "##### Cs = Increasing this increases penalty and the affect of regularization because it shrinks the contact area.\n",
    "##### CV = CrossValidations or number of folds\n",
    "##### Penalty = Regularization Tactic, l1 - LASSO, l2 - Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=15, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "           refit=True, scoring='accuracy', solver='liblinear', tol=0.0001,\n",
       "           verbose=0)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TTS our data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 12,)\n",
    "\n",
    "# Lets set our model parameters \n",
    "logreg_cv = LogisticRegressionCV(Cs=15, cv=5, penalty='l1', scoring='accuracy', solver='liblinear')\n",
    "logreg_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C for class:\n",
      "{'non-violent': 0.071968567300115138, 'violent': 0.26826957952797248, 'non-crime': 2682.695795279722}\n"
     ]
    }
   ],
   "source": [
    "# find best C per class  \n",
    "print('best C for class:')\n",
    "\n",
    "#Building a dictionary that does a regression for each of the Y classes\n",
    "# after the fit it grabs the C value for said logistic regression and puts them together.\n",
    "best_C = {logreg_cv.classes_[i]:x for i, (x, c) in enumerate(zip(logreg_cv.C_, logreg_cv.classes_))}\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2682.695795279722, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit regular logit model to 'non-crime', 'non-violent', and 'violent' classes\n",
    "# use lasso penalty\n",
    "logreg_1 = LogisticRegression(C=best_C['non-crime'], penalty='l1', solver='liblinear', multi_class = 'ovr')\n",
    "logreg_2 = LogisticRegression(C=best_C['non-violent'], penalty='l1', solver='liblinear', multi_class = 'ovr')\n",
    "logreg_3 = LogisticRegression(C=best_C['violent'], penalty='l1', solver='liblinear', multi_class = 'ovr')\n",
    "\n",
    "# Lets check out all of our outputs for all of our models\n",
    "# Non Crimes\n",
    "logreg_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.071968567300115138, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non Violent\n",
    "logreg_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.26826957952797248, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Violent\n",
    "logreg_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build confusion matrices for the models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             non-crime  non-violent  violent\n",
      "non-crime           67          172      655\n",
      "non-violent         15          419      449\n",
      "violent             17          338     2368\n",
      "             non-crime  non-violent  violent\n",
      "non-crime           57          197      640\n",
      "non-violent         15          425      443\n",
      "violent             15          362     2346\n",
      "             non-crime  non-violent  violent\n",
      "non-crime           65          176      653\n",
      "non-violent         15          417      451\n",
      "violent             17          342     2364\n"
     ]
    }
   ],
   "source": [
    "# using our logregs to predict on our test set and storing predictions\n",
    "Y_1_pred = logreg_1.predict(X_test)\n",
    "Y_2_pred = logreg_2.predict(X_test)\n",
    "Y_3_pred = logreg_3.predict(X_test)\n",
    "\n",
    "# stores confusion matrix for Y Test and Y Pred  \n",
    "conmat_1 = confusion_matrix(y_test, Y_1_pred, labels=logreg_1.classes_)\n",
    "# converts np.matrix format matrix to a dataframe and adds index and column names\n",
    "conmat_1 = pd.DataFrame(conmat_1, columns=logreg_1.classes_, index=logreg_1.classes_)\n",
    "\n",
    "conmat_2 = confusion_matrix(y_test, Y_2_pred, labels=logreg_2.classes_)\n",
    "conmat_2 = pd.DataFrame(conmat_2, columns=logreg_2.classes_, index=logreg_2.classes_)\n",
    "\n",
    "conmat_3 = confusion_matrix(y_test, Y_3_pred, labels=logreg_3.classes_)\n",
    "conmat_3 = pd.DataFrame(conmat_3, columns=logreg_3.classes_, index=logreg_3.classes_)\n",
    "\n",
    "print conmat_1\n",
    "print conmat_2\n",
    "print conmat_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretting multiclass confussion matrix is a bit different from a single class.  With single class the options are 'Is' and 'Is Not' or 'True' and 'False'.  With a multiclass confusion matrix our cells in our matrix can represent more than just a single outcome (\"true Positive\", \"True Negative\",\"False Positive\", \"False\" Negative\").\n",
    "\n",
    "Lets take a look at the First confusion Matrix.\n",
    "There is a clear trend of True Positives along the top-left to bottom-right diagonal, perfectly classified data, however if we are only interested in how Violent crimes are classified we can see that there are 2368 True Positives at the intersection of Violent and Violent.  We can also see that every value that did not make it into a violent row or column is technically a True Negative with respect to looking at the classification of violent crimes.  This is so because with respect to violent crimes, all of those crimes in that area are Truely not Violent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  non-crime       0.68      0.07      0.13       894\n",
      "non-violent       0.45      0.47      0.46       883\n",
      "    violent       0.68      0.87      0.76      2723\n",
      "\n",
      "avg / total       0.64      0.63      0.58      4500\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  non-crime       0.66      0.06      0.12       894\n",
      "non-violent       0.43      0.48      0.46       883\n",
      "    violent       0.68      0.86      0.76      2723\n",
      "\n",
      "avg / total       0.63      0.63      0.57      4500\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  non-crime       0.67      0.07      0.13       894\n",
      "non-violent       0.45      0.47      0.46       883\n",
      "    violent       0.68      0.87      0.76      2723\n",
      "\n",
      "avg / total       0.63      0.63      0.58      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test, Y_1_pred))\n",
    "print(classification_report(y_test, Y_2_pred))\n",
    "print(classification_report(y_test, Y_3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Precision ( True Positives divided by Total Predicted Positives)\n",
    "Of our First models True predictions for non-crime classification, 68% of them were correct(True Positive) and the other 32% of them were incorrect (False Positive).\n",
    "\n",
    "\n",
    "#### - Recall (True Positives divided by Total Actual Positives)\n",
    "of our Second models True Predictions it correctly predicted 48% of the total Positives for non-violent crimes.\n",
    "\n",
    "\n",
    "#### - f1-score ( 2 * (precision * recall) / (precision + recall) )\n",
    "This is the weighted average of the Precision and Recall.\n",
    "For the Third Models Violent predictions.   f1 = 2 x (0.68 x 0.87)/(0.68+0.87)\n",
    "\n",
    "\n",
    "#### - Support -  Number of True Values in said class\n",
    "Non-Crime = 894\n",
    "Non-Violent = 883\n",
    "Violent = 2723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run gridsearch using GridSearchCV and 5 folds\n",
    "# score on accuracy; what does this metric tell us?\n",
    "logreg = LogisticRegression()\n",
    "C_vals = [0.0001, 0.001, 0.01, 0.1, 0.5, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]\n",
    "penalties = ['l1','l2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we gridsearch in Sklearn, we pass possible values we want checked as a dictionary. Keys are the parameter names and values are lists of values we want checked.  Apart from the first parameter (outside the dictionary) which is our model.\n",
    "\n",
    "As GridSearch is comparing all of our parameters against eachother we dont have to specify any when declaring the model.\n",
    "\n",
    "In this Example we are only Gridsearching Regression Types and C Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   15.9s\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1_macro',\n",
       "       verbose=True)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W.\n",
    "gs = GridSearchCV(logreg, {'penalty':penalties, 'C':C_vals}, verbose=True, cv=5, scoring='f1_macro')\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10.0, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the best parameters of our gridsearch model.\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  non-crime       0.63      0.08      0.14       920\n",
      "non-violent       0.46      0.46      0.46       883\n",
      "    violent       0.68      0.88      0.77      2697\n",
      "\n",
      "avg / total       0.63      0.63      0.58      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use this parameter to .fit, .predict, and print a classification_report for our X and Y\n",
    "lr = LogisticRegression(penalty = 'l1', C = 10.0)\n",
    "\n",
    "X_train_gs, X_test_gs, y_train_gs, y_test_gs = train_test_split(X,y, random_state = 177)\n",
    "\n",
    "lr.fit(X_train_gs, y_train_gs)\n",
    "\n",
    "y_pred_gs = lr.predict(X_test_gs)\n",
    "\n",
    "print classification_report(y_test_gs, y_pred_gs)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
